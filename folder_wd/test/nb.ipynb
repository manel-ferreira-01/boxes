{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import threading\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import json\n",
    "import folium\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_gps_from_video(video_path):\n",
    "    \"\"\"Extract single GPS coordinate (latitude, longitude) from video metadata.\"\"\"\n",
    "    cmd = [\n",
    "        \"ffprobe\",\n",
    "        \"-v\", \"quiet\",\n",
    "        \"-print_format\", \"json\",\n",
    "        \"-show_entries\", \"format_tags\",\n",
    "        \"-show_entries\", \"stream_tags\",\n",
    "        video_path\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    metadata = json.loads(result.stdout or \"{}\")\n",
    "\n",
    "    # Search for location info in both format and stream tags\n",
    "    tags_candidates = []\n",
    "    format_tags = metadata.get(\"format\", {}).get(\"tags\", {})\n",
    "    if format_tags:\n",
    "        tags_candidates.append(format_tags)\n",
    "\n",
    "    streams = metadata.get(\"streams\", [])\n",
    "    for s in streams:\n",
    "        if \"tags\" in s:\n",
    "            tags_candidates.append(s[\"tags\"])\n",
    "\n",
    "    gps_raw = None\n",
    "    for tags in tags_candidates:\n",
    "        for key in [\"com.apple.quicktime.location.ISO6709\", \"location\", \"location-eng\"]:\n",
    "            if key in tags:\n",
    "                gps_raw = tags[key]\n",
    "                break\n",
    "        if gps_raw:\n",
    "            break\n",
    "\n",
    "    if not gps_raw:\n",
    "        return None\n",
    "\n",
    "    # Normalize and extract coordinates\n",
    "    match = re.match(r\"([+-]\\d+\\.\\d+)([+-]\\d+\\.\\d+)\", gps_raw)\n",
    "    if match:\n",
    "        lat = float(match.group(1))\n",
    "        lon = float(match.group(2))\n",
    "        return lat, lon\n",
    "\n",
    "    return None\n",
    "\n",
    "def overlay_masks_on_image(image, masks, labels, alpha=0.4, show_labels=True):\n",
    "    \"\"\"Overlay segmentation masks on an image with random colors and centered labels.\"\"\"\n",
    "    overlay = image.copy()\n",
    "    color_map = {}\n",
    "\n",
    "    # Assign each label a random color (consistent seed)\n",
    "    rng = np.random.default_rng(42)\n",
    "    for label in np.unique(labels):\n",
    "        color_map[label] = rng.integers(0, 255, size=3, dtype=np.uint8)\n",
    "\n",
    "    # Auto-adjust font scale to image size\n",
    "    h, w = image.shape[:2]\n",
    "    base_font_scale = max(0.6, min(1.5, (w * h) / (1920 * 1080) * 1.0))\n",
    "    font_thickness = int(max(1, base_font_scale * 2))\n",
    "\n",
    "    for mask, label in zip(masks, labels):\n",
    "        color = color_map[label]\n",
    "        mask = mask.astype(bool)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "\n",
    "        # Blend per channel\n",
    "        for c in range(3):\n",
    "            overlay[:, :, c][mask] = (\n",
    "                (1 - alpha) * overlay[:, :, c][mask] + alpha * color[c]\n",
    "            ).astype(np.uint8)\n",
    "\n",
    "        # --- Compute centroid for label placement ---\n",
    "        if show_labels:\n",
    "            M = cv2.moments(mask.astype(np.uint8))\n",
    "            if M[\"m00\"] > 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "                # Draw text with outline\n",
    "                cv2.putText(\n",
    "                    overlay,\n",
    "                    label,\n",
    "                    (cx - 20, cy),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    base_font_scale,\n",
    "                    (255, 255, 255),\n",
    "                    font_thickness + 1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    overlay,\n",
    "                    label,\n",
    "                    (cx - 20, cy),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    base_font_scale,\n",
    "                    (0, 0, 0),\n",
    "                    font_thickness,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    # --- Create legend on the right ---\n",
    "    legend = np.ones((overlay.shape[0], 200, 3), dtype=np.uint8) * 255\n",
    "    y = 30\n",
    "    for label, color in color_map.items():\n",
    "        cv2.rectangle(legend, (10, y - 15), (40, y + 5), color.tolist(), -1)\n",
    "        cv2.putText(legend, label, (50, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "        y += 25\n",
    "\n",
    "    combined = np.hstack((overlay, legend))\n",
    "    return combined\n",
    "\n",
    "\n",
    "\n",
    "def hist_per_video(video_path, labels_of_interest):\n",
    "    \"\"\"\n",
    "    Compute per-label area from SAM masks and generate overlays + histogram for all frames.\n",
    "    Also saves a CSV with per-class percentage area coverage.\n",
    "    \"\"\"\n",
    "    orig_dir = os.path.join(video_path, \"original\")\n",
    "    lang_sam_dir = os.path.join(video_path, \"lang_sam\")\n",
    "\n",
    "    if not (os.path.isdir(orig_dir) and os.path.isdir(lang_sam_dir)):\n",
    "        print(f\"‚ö†Ô∏è Missing 'original/' or 'lang_sam/' in {video_path}\")\n",
    "        return 0\n",
    "\n",
    "    pkl_files = sorted([f for f in os.listdir(lang_sam_dir) if f.endswith(\".pkl\")])\n",
    "    if not pkl_files:\n",
    "        print(f\"‚ö†Ô∏è No pickle files in {lang_sam_dir}\")\n",
    "        return 0\n",
    "\n",
    "    area_dict = {}\n",
    "    total_area = 0\n",
    "\n",
    "    # --- Process each frame ---\n",
    "    for idx, pkl_name in enumerate(pkl_files):\n",
    "        pkl_path = os.path.join(lang_sam_dir, pkl_name)\n",
    "\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            output = pickle.load(f)\n",
    "\n",
    "        # Skip frames without valid segmentation\n",
    "        if not isinstance(output[0].get(\"masks\"), np.ndarray):\n",
    "            print(f\"‚ö†Ô∏è No masks found in {pkl_name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        masks = output[0][\"masks\"].astype(bool)\n",
    "        labels = output[0][\"labels\"]\n",
    "\n",
    "        # --- Find corresponding image ---\n",
    "        frame_num = re.findall(r\"\\d+\", pkl_name)\n",
    "        frame_num = frame_num[-1] if frame_num else f\"{idx:05d}\"\n",
    "        img_candidates = sorted(\n",
    "            [f for f in os.listdir(orig_dir) if frame_num in f and f.lower().endswith((\".jpg\", \".png\"))]\n",
    "        )\n",
    "        if not img_candidates:\n",
    "            all_imgs = sorted([f for f in os.listdir(orig_dir) if f.lower().endswith((\".jpg\", \".png\"))])\n",
    "            if idx < len(all_imgs):\n",
    "                img_candidates = [all_imgs[idx]]\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Could not match frame for {pkl_name}\")\n",
    "                continue\n",
    "\n",
    "        img_path = os.path.join(orig_dir, img_candidates[0])\n",
    "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # --- Overlay visualization ---\n",
    "        #overlay = overlay_masks_on_image(image, masks, labels)\n",
    "        #overlay_filename = f\"seg_overlay_{frame_num}.png\"\n",
    "        #overlay_path = os.path.join(video_path, \"segmentation_viz\", overlay_filename)\n",
    "        #os.makedirs(os.path.dirname(overlay_path), exist_ok=True)\n",
    "        #cv2.imwrite(overlay_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "        #print(f\"üñºÔ∏è Saved overlay {overlay_filename}\")\n",
    "\n",
    "        # --- Compute per-label area ---\n",
    "        for label, mask in zip(labels, masks):\n",
    "            area = np.sum(mask)\n",
    "            area_dict[label] = area_dict.get(label, 0) + area\n",
    "            total_area += area\n",
    "\n",
    "    # --- Compute relative (percentage) areas ---\n",
    "    if total_area == 0:\n",
    "        print(f\"‚ö†Ô∏è No valid mask pixels in {video_path}\")\n",
    "        return 0\n",
    "\n",
    "    area_percent = {label: (area / total_area * 100) for label, area in area_dict.items()}\n",
    "\n",
    "    # --- Plot histogram (percentages) ---\n",
    "    plt.figure()\n",
    "    plt.bar(area_percent.keys(), area_percent.values(), color='skyblue')\n",
    "    plt.ylabel(\"Percentage of segmented area (%)\")\n",
    "    plt.title(f\"Area percentage per label for {os.path.basename(video_path)}\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    hist_path = os.path.join(video_path, \"area_hist.png\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "    print(f\"üìä Saved histogram to {hist_path}\")\n",
    "\n",
    "    # --- Save CSV with percentages ---\n",
    "    import csv\n",
    "    csv_path = os.path.join(video_path,\"..\", \"segmentation_stats.csv\")\n",
    "    all_labels =  [\"grass\", \"trees\", \"bus\", \"building\",\"people\",\"car\",\"road\"]\n",
    "    values = [f\"{area_percent[l]:.3f}\" for l in labels]\n",
    "    file_exists = os.path.exists(csv_path)\n",
    "\n",
    "    with open(csv_path, \"a+\", newline=\"\") as f:\n",
    "        f.seek(0)\n",
    "        existing_rows = [row[0] for row in csv.reader(f)]\n",
    "        writer = csv.writer(f)\n",
    "        # Write header only once\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"video\"] + all_labels)\n",
    "        # write the values in order of the corredsponding labels\n",
    "        if os.path.basename(video_path) not in existing_rows:\n",
    "            #writer.writerow([os.path.basename(video_path)] + values)\n",
    "            writer.writerow([os.path.basename(video_path)] + [f\"{area_percent.get(label, 0):.3f}\" for label in all_labels])\n",
    "\n",
    "    print(f\"üíæ Created and saved histogram data to {csv_path}\")\n",
    "\n",
    "    # --- Return total percentage for labels of interest ---\n",
    "    return sum(area_percent.get(label, 0) for label in labels_of_interest)\n",
    "\n",
    "\n",
    "def map_videos(video_folders, labels_of_interest=[\"trees\", \"grass\"],out_filename=\"map.html\", ball_clr=\"green\"):\n",
    "    \"\"\"Plot circle markers for each video with radius ranked by tree area.\"\"\"\n",
    "    m = folium.Map(location=[38.736, -9.14], zoom_start=12)\n",
    "\n",
    "    # Step 1: Gather data\n",
    "    video_data = []\n",
    "    for video_name in tqdm(video_folders):\n",
    "        video_basename = os.path.basename(video_name)\n",
    "        group_dir = os.path.dirname(os.path.dirname(video_name))  # e.g., g1/output/topo_alameda ‚Üí g1/\n",
    "        video_path = os.path.join(group_dir, f\"{video_basename}.MOV\")\n",
    "\n",
    "        # fallback: try .mp4 if .MOV not found\n",
    "        if not os.path.exists(video_path):\n",
    "            video_path = os.path.join(group_dir, f\"{video_basename}.mp4\")\n",
    "\n",
    "        gps = get_gps_from_video(video_path)\n",
    "        area = hist_per_video(video_name, labels_of_interest)\n",
    "\n",
    "        if video_name == \"topo_alameda\":\n",
    "            gps = (38.737340, -9.129204)\n",
    "        elif gps is None:\n",
    "            print(f\"No GPS found for {video_name}\")\n",
    "            continue\n",
    "\n",
    "        # --- Save CSV with percentages ---\n",
    "        import csv\n",
    "        csv_path = os.path.join(video_name,\"..\", \"gps.csv\")\n",
    "        file_exists = os.path.exists(csv_path)\n",
    "\n",
    "        with open(csv_path, \"a+\", newline=\"\") as f:\n",
    "            f.seek(0)\n",
    "            existing_rows = [row[0] for row in csv.reader(f)]\n",
    "            writer = csv.writer(f)\n",
    "            # Write header only once\n",
    "            if not file_exists:\n",
    "                writer.writerow([\"video\"] + [\"lat\"] + [\"lon\"])\n",
    "            # Write only if the video name is not already in the file\n",
    "            if os.path.basename(video_name) not in existing_rows:\n",
    "                writer.writerow([os.path.basename(video_name)] + [gps[0]] + [gps[1]])\n",
    "\n",
    "        print(f\"üíæ Created and saved gps data to {csv_path}\")\n",
    "\n",
    "        video_data.append((video_name, gps, area))\n",
    "\n",
    "    if not video_data:\n",
    "        print(\"‚ö†Ô∏è No valid videos found.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Rank / normalize areas\n",
    "    areas = np.array([a for _, _, a in video_data])\n",
    "    min_area, max_area = np.min(areas), np.max(areas)\n",
    "    if max_area == min_area:\n",
    "        normalized = np.ones_like(areas)\n",
    "    else:\n",
    "        normalized = (areas - min_area) / (max_area - min_area)\n",
    "\n",
    "    # Step 3: Map normalized areas to a visual radius range\n",
    "    min_radius, max_radius = 10, 40\n",
    "    radii = min_radius + normalized * (max_radius - min_radius)\n",
    "\n",
    "    # Step 4: Plot circles with ranked sizes\n",
    "    for (video_name, (lat, lon), area), radius in zip(video_data, radii):\n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lon],\n",
    "            radius=radius,\n",
    "            color=ball_clr,\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "            popup=f\"{video_name}: {int(area)} px¬≤ trees\",\n",
    "        ).add_to(m)\n",
    "\n",
    "    m.save(out_filename)\n",
    "    print(f\"‚úÖ Saved map to {out_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \".\"\n",
    "#groups = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"g\")]\n",
    "groups = [\"g1\",\"g2\",\"g3\",\"g4\",\"g5\",\"g6\"]\n",
    "\n",
    "for group in groups:\n",
    "    output_root = os.path.join(base_dir, group, \"output\")\n",
    "    if not os.path.isdir(output_root):\n",
    "        continue\n",
    "\n",
    "    # Collect all per-video output folders inside each group's output/\n",
    "    video_folders = [\n",
    "        os.path.join(output_root, d)\n",
    "        for d in os.listdir(output_root)\n",
    "        if os.path.isdir(os.path.join(output_root, d))\n",
    "    ]\n",
    "\n",
    "    if not video_folders:\n",
    "        print(f\"‚ö†Ô∏è No video outputs found in {group}/output\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÇ Processing {group}: {len(video_folders)} videos\")\n",
    "\n",
    "    # Run vegetation map\n",
    "    map_videos(\n",
    "        video_folders,\n",
    "        labels_of_interest=[\"trees\", \"grass\"],\n",
    "        out_filename=os.path.join(output_root, f\"{group}_green_map.html\"),\n",
    "        ball_clr=\"green\",\n",
    "    )\n",
    "\n",
    "    # Run urban map\n",
    "    map_videos(\n",
    "        video_folders,\n",
    "        labels_of_interest=[\"road\", \"car\", \"bus\", \"building\"],\n",
    "        out_filename=os.path.join(output_root, f\"{group}_urban_map.html\"),\n",
    "        ball_clr=\"grey\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"./g1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install k3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import k3d\n",
    "\n",
    "def visualize_vggt_pointcloud(video_path, downsample=10, conf_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Visualize VGGT 3D point cloud for a given video output folder.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): e.g. './g1/output/topo_alameda'\n",
    "        downsample (int): keep 1 every N points\n",
    "        conf_thresh (float): confidence threshold on world_points_conf\n",
    "    \"\"\"\n",
    "    vggt_dir = os.path.join(video_path, \"vggt\")\n",
    "    if not os.path.isdir(vggt_dir):\n",
    "        print(f\"‚ö†Ô∏è No VGGT folder in {video_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load the latest (or largest) pickle file\n",
    "    pkl_files = sorted(\n",
    "        [f for f in os.listdir(vggt_dir) if f.endswith(\".pkl\")],\n",
    "        key=lambda x: os.path.getsize(os.path.join(vggt_dir, x)),\n",
    "        reverse=True\n",
    "    )\n",
    "    if not pkl_files:\n",
    "        print(f\"‚ö†Ô∏è No VGGT pickle found in {vggt_dir}\")\n",
    "        return None\n",
    "\n",
    "    latest_pkl = os.path.join(vggt_dir, pkl_files[0])\n",
    "    print(f\"üì¶ Loading VGGT data from {latest_pkl}\")\n",
    "\n",
    "    with open(latest_pkl, \"rb\") as f:\n",
    "        vggt_data = pickle.load(f)\n",
    "\n",
    "    # --- Extract fields safely ---\n",
    "    world_points = np.array(vggt_data.get(\"world_points\") or vggt_data.get(\"wrld_points\"))\n",
    "    if world_points is None or world_points.size == 0:\n",
    "        print(f\"‚ö†Ô∏è No 'world_points' field found in {latest_pkl}\")\n",
    "        return None\n",
    "\n",
    "    conf = np.array(vggt_data.get(\"world_points_conf\", np.ones(world_points.shape[:-1])))\n",
    "    conf = np.squeeze(conf)\n",
    "\n",
    "    # --- Apply confidence mask ---\n",
    "    mask_conf = conf > conf_thresh\n",
    "    world_points = np.reshape(world_points, (-1, 3))[mask_conf.flatten(), :]\n",
    "\n",
    "    # --- Handle RGB ---\n",
    "    images = vggt_data.get(\"images\")\n",
    "    if images is not None:\n",
    "        # images: (N, C, H, W) ‚Üí (H*W*N, 3)\n",
    "        rgb = np.transpose(images, (0, 2, 3, 1)).reshape(-1, 3)\n",
    "        rgb = np.clip(rgb * 255.0, 0, 255).astype(np.uint8)[mask_conf.flatten(), :]\n",
    "    else:\n",
    "        rgb = np.full((world_points.shape[0], 3), 200, dtype=np.uint8)  # gray fallback\n",
    "\n",
    "    # --- Downsample for speed ---\n",
    "    if downsample > 1:\n",
    "        world_points = world_points[::downsample]\n",
    "        rgb = rgb[::downsample]\n",
    "\n",
    "    # --- Convert to integer RGB for K3D ---\n",
    "    rgb_int = (\n",
    "        (rgb[:, 0].astype(np.uint32) << 16) +\n",
    "        (rgb[:, 1].astype(np.uint32) << 8) +\n",
    "        rgb[:, 2].astype(np.uint32)\n",
    "    )\n",
    "\n",
    "    # --- Visualize in K3D ---\n",
    "    plot = k3d.plot(height=600)\n",
    "    point_cloud = k3d.points(\n",
    "        positions=world_points,\n",
    "        colors=rgb_int,\n",
    "        point_size=0.005,\n",
    "        shader='3d'\n",
    "    )\n",
    "    plot += point_cloud\n",
    "    plot.display()\n",
    "\n",
    "    print(f\"‚úÖ Displayed {len(world_points):,} points from {os.path.basename(video_path)}\")\n",
    "    return plot\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "video = \"./g1/output/almirante_reis\"\n",
    "plot = visualize_vggt_pointcloud(video, downsample=10, conf_thresh=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_objects_in_video(video_yolo_json_dir, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Count unique detected objects per class in a YOLO JSON folder,\n",
    "    considering only detections above a confidence threshold.\n",
    "    \"\"\"\n",
    "    counts = defaultdict(set)  # {class_name: set(track_ids)}\n",
    "    json_files = sorted([f for f in os.listdir(video_yolo_json_dir) if f.endswith(\".json\")])\n",
    "\n",
    "    for jf in json_files:\n",
    "        jf_path = os.path.join(video_yolo_json_dir, jf)\n",
    "        try:\n",
    "            with open(jf_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read {jf_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if not data or not isinstance(data, list):\n",
    "            continue\n",
    "\n",
    "        detections = data[0] if isinstance(data[0], list) else data\n",
    "        for det in detections:\n",
    "            conf = det.get(\"confidence\", 0.0)\n",
    "            if conf < conf_threshold:\n",
    "                continue\n",
    "\n",
    "            cls = det.get(\"class_name\", \"unknown\")\n",
    "            tid = det.get(\"track_id\", None)\n",
    "\n",
    "            # Handle missing or invalid tracking IDs\n",
    "            if tid is None or tid == -1:\n",
    "                tid = f\"{jf}_{cls}\"  # fallback unique id per frame\n",
    "\n",
    "            counts[cls].add(tid)\n",
    "\n",
    "    return {cls: len(tids) for cls, tids in counts.items()}\n",
    "\n",
    "def process_group(group_dir, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Process all videos inside a group/output/ folder and save a CSV there.\n",
    "    \"\"\"\n",
    "    output_dir = os.path.join(group_dir, \"output\")\n",
    "    if not os.path.isdir(output_dir):\n",
    "        print(f\"‚ö†Ô∏è No output folder in {group_dir}\")\n",
    "        return\n",
    "\n",
    "    video_stats = {}\n",
    "    for video_name in os.listdir(output_dir):\n",
    "        video_dir = os.path.join(output_dir, video_name)\n",
    "        yolo_json_dir = os.path.join(video_dir, \"yolo_json\")\n",
    "        if not os.path.isdir(yolo_json_dir):\n",
    "            continue\n",
    "\n",
    "        print(f\"üì¶ Processing {group_dir}/{video_name} ...\")\n",
    "        counts = count_objects_in_video(yolo_json_dir, conf_threshold)\n",
    "        if counts:\n",
    "            video_stats[video_name] = counts\n",
    "\n",
    "    if not video_stats:\n",
    "        print(f\"‚ö†Ô∏è No valid YOLO data found in {group_dir}\")\n",
    "        return\n",
    "\n",
    "    # Normalize into DataFrame\n",
    "    df = pd.DataFrame.from_dict(video_stats, orient=\"index\").fillna(0).astype(int)\n",
    "    df.index.name = \"video_id\"\n",
    "    df.sort_index(inplace=True)\n",
    "    df[\"total_objects\"] = df.sum(axis=1)\n",
    "\n",
    "    csv_path = os.path.join(output_dir, f\"yolo_counts.csv\")\n",
    "    df.to_csv(csv_path)\n",
    "    print(f\"‚úÖ Saved {csv_path}\")\n",
    "    return df\n",
    "\n",
    "def merge_all_groups(base_dir=\".\", conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Loop over all group folders (g1, g2, g5-nogps, etc.) and create one CSV per group.\n",
    "    \"\"\"\n",
    "    for group in sorted(os.listdir(base_dir)):\n",
    "        group_path = os.path.join(base_dir, group)\n",
    "        if not os.path.isdir(group_path) or not group.startswith(\"g\"):\n",
    "            continue\n",
    "        process_group(group_path, conf_threshold)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_all_groups(\".\", conf_threshold=0.65)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
