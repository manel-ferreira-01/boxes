{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import threading\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import json\n",
    "import folium\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "global df\n",
    "\n",
    "\n",
    "def from_dataframe_to_gps(video_path):\n",
    "    \"\"\"\n",
    "    Return (latitude, longitude) for a video, using the parsed coordinates DataFrame.\n",
    "    Falls back to None if the video number isn't found.\n",
    "    \"\"\"\n",
    "    # Extract video number from filename, e.g. \"video12.mp4\" or \"V√≠deo_12.mov\"\n",
    "    basename = os.path.basename(video_path)\n",
    "    match = re.search(r'(\\d+)', basename)\n",
    "    if not match:\n",
    "        print(f\"‚ö†Ô∏è  Could not extract video number from {basename}\")\n",
    "        return None\n",
    "\n",
    "    video_num = int(match.group(1))\n",
    "\n",
    "    # Find matching row in DataFrame\n",
    "    row = df.loc[df['video'] == video_num]\n",
    "    if row.empty:\n",
    "        print(f\"‚ö†Ô∏è  No coordinates found for video {video_num}\")\n",
    "        return None\n",
    "\n",
    "    lat = float(row.iloc[0]['latitude'])\n",
    "    lon = float(row.iloc[0]['longitude'])\n",
    "    return lat, lon\n",
    "\n",
    "def get_gps_from_video(video_path):\n",
    "    \"\"\"Extract single GPS coordinate (latitude, longitude) from video metadata.\"\"\"\n",
    "    cmd = [\n",
    "        \"ffprobe\",\n",
    "        \"-v\", \"quiet\",\n",
    "        \"-print_format\", \"json\",\n",
    "        \"-show_entries\", \"format_tags\",\n",
    "        \"-show_entries\", \"stream_tags\",\n",
    "        video_path\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    print(video_path)\n",
    "    metadata = json.loads(result.stdout or \"{}\")\n",
    "\n",
    "    # Search for location info in both format and stream tags\n",
    "    tags_candidates = []\n",
    "    format_tags = metadata.get(\"format\", {}).get(\"tags\", {})\n",
    "    if format_tags:\n",
    "        tags_candidates.append(format_tags)\n",
    "\n",
    "    streams = metadata.get(\"streams\", [])\n",
    "    for s in streams:\n",
    "        if \"tags\" in s:\n",
    "            tags_candidates.append(s[\"tags\"])\n",
    "\n",
    "    gps_raw = None\n",
    "    for tags in tags_candidates:\n",
    "        for key in [\"com.apple.quicktime.location.ISO6709\", \"location\", \"location-eng\"]:\n",
    "            if key in tags:\n",
    "                gps_raw = tags[key]\n",
    "                break\n",
    "        if gps_raw:\n",
    "            break\n",
    "\n",
    "    if not gps_raw:\n",
    "        return None\n",
    "\n",
    "    # Normalize and extract coordinates\n",
    "    match = re.match(r\"([+-]\\d+\\.\\d+)([+-]\\d+\\.\\d+)\", gps_raw)\n",
    "    if match:\n",
    "        lat = float(match.group(1))\n",
    "        lon = float(match.group(2))\n",
    "        return lat, lon\n",
    "\n",
    "    return None\n",
    "\n",
    "def overlay_masks_on_image(image, masks, labels, alpha=0.4, show_labels=True):\n",
    "    \"\"\"Overlay segmentation masks on an image with random colors and centered labels.\"\"\"\n",
    "    overlay = image.copy()\n",
    "    color_map = {}\n",
    "\n",
    "    # Assign each label a random color (consistent seed)\n",
    "    rng = np.random.default_rng(42)\n",
    "    for label in np.unique(labels):\n",
    "        color_map[label] = rng.integers(0, 255, size=3, dtype=np.uint8)\n",
    "\n",
    "    # Auto-adjust font scale to image size\n",
    "    h, w = image.shape[:2]\n",
    "    base_font_scale = max(0.6, min(1.5, (w * h) / (1920 * 1080) * 1.0))\n",
    "    font_thickness = int(max(1, base_font_scale * 2))\n",
    "\n",
    "    for mask, label in zip(masks, labels):\n",
    "        color = color_map[label]\n",
    "        mask = mask.astype(bool)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "\n",
    "        # Blend per channel\n",
    "        for c in range(3):\n",
    "            overlay[:, :, c][mask] = (\n",
    "                (1 - alpha) * overlay[:, :, c][mask] + alpha * color[c]\n",
    "            ).astype(np.uint8)\n",
    "\n",
    "        # --- Compute centroid for label placement ---\n",
    "        if show_labels:\n",
    "            M = cv2.moments(mask.astype(np.uint8))\n",
    "            if M[\"m00\"] > 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "                # Draw text with outline\n",
    "                cv2.putText(\n",
    "                    overlay,\n",
    "                    label,\n",
    "                    (cx - 20, cy),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    base_font_scale,\n",
    "                    (255, 255, 255),\n",
    "                    font_thickness + 1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    overlay,\n",
    "                    label,\n",
    "                    (cx - 20, cy),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    base_font_scale,\n",
    "                    (0, 0, 0),\n",
    "                    font_thickness,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    # --- Create legend on the right ---\n",
    "    legend = np.ones((overlay.shape[0], 200, 3), dtype=np.uint8) * 255\n",
    "    y = 30\n",
    "    for label, color in color_map.items():\n",
    "        cv2.rectangle(legend, (10, y - 15), (40, y + 5), color.tolist(), -1)\n",
    "        cv2.putText(legend, label, (50, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "        y += 25\n",
    "\n",
    "    combined = np.hstack((overlay, legend))\n",
    "    return combined\n",
    "\n",
    "\n",
    "\n",
    "def hist_per_video(video_path, labels_of_interest):\n",
    "    \"\"\"\n",
    "    Compute per-label area from SAM masks and generate overlays + histogram for all frames.\n",
    "    Also saves a CSV with per-class percentage area coverage.\n",
    "    \"\"\"\n",
    "    orig_dir = os.path.join(video_path, \"original\")\n",
    "    lang_sam_dir = os.path.join(video_path, \"lang_sam\")\n",
    "\n",
    "    if not (os.path.isdir(orig_dir) and os.path.isdir(lang_sam_dir)):\n",
    "        print(f\"‚ö†Ô∏è Missing 'original/' or 'lang_sam/' in {video_path}\")\n",
    "        return 0\n",
    "\n",
    "    pkl_files = sorted([f for f in os.listdir(lang_sam_dir) if f.endswith(\".pkl\")])\n",
    "    if not pkl_files:\n",
    "        print(f\"‚ö†Ô∏è No pickle files in {lang_sam_dir}\")\n",
    "        return 0\n",
    "\n",
    "    area_dict = {}\n",
    "    total_area = 0\n",
    "\n",
    "    # --- Process each frame ---\n",
    "    for idx, pkl_name in enumerate(pkl_files):\n",
    "        pkl_path = os.path.join(lang_sam_dir, pkl_name)\n",
    "\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            output = pickle.load(f)\n",
    "\n",
    "        # Skip frames without valid segmentation\n",
    "        if not isinstance(output[0].get(\"masks\"), np.ndarray):\n",
    "            print(f\"‚ö†Ô∏è No masks found in {pkl_name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        masks = output[0][\"masks\"].astype(bool)\n",
    "        labels = output[0][\"labels\"]\n",
    "\n",
    "        # --- Find corresponding image ---\n",
    "        frame_num = re.findall(r\"\\d+\", pkl_name)\n",
    "        frame_num = frame_num[-1] if frame_num else f\"{idx:05d}\"\n",
    "        img_candidates = sorted(\n",
    "            [f for f in os.listdir(orig_dir) if frame_num in f and f.lower().endswith((\".jpg\", \".png\"))]\n",
    "        )\n",
    "        if not img_candidates:\n",
    "            all_imgs = sorted([f for f in os.listdir(orig_dir) if f.lower().endswith((\".jpg\", \".png\"))])\n",
    "            if idx < len(all_imgs):\n",
    "                img_candidates = [all_imgs[idx]]\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Could not match frame for {pkl_name}\")\n",
    "                continue\n",
    "\n",
    "        img_path = os.path.join(orig_dir, img_candidates[0])\n",
    "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # --- Overlay visualization ---\n",
    "        #overlay = overlay_masks_on_image(image, masks, labels)\n",
    "        #overlay_filename = f\"seg_overlay_{frame_num}.png\"\n",
    "        #overlay_path = os.path.join(video_path, \"segmentation_viz\", overlay_filename)\n",
    "        #print(overlay_path)\n",
    "        #os.makedirs(os.path.dirname(overlay_path), exist_ok=True)\n",
    "        #cv2.imwrite(overlay_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "        #print(f\"üñºÔ∏è Saved overlay {overlay_filename}\")\n",
    "\n",
    "        # --- Compute per-label area ---\n",
    "        for label, mask in zip(labels, masks):\n",
    "            area = np.sum(mask)\n",
    "            area_dict[label] = area_dict.get(label, 0) + area\n",
    "            total_area += area\n",
    "\n",
    "    # --- Compute relative (percentage) areas ---\n",
    "    if total_area == 0:\n",
    "        print(f\"‚ö†Ô∏è No valid mask pixels in {video_path}\")\n",
    "        return 0\n",
    "\n",
    "    area_percent = {label: (area / total_area * 100) for label, area in area_dict.items()}\n",
    "\n",
    "    # --- Plot histogram (percentages) ---\n",
    "    plt.figure()\n",
    "    plt.bar(area_percent.keys(), area_percent.values(), color='skyblue')\n",
    "    plt.ylabel(\"Percentage of segmented area (%)\")\n",
    "    plt.title(f\"Area percentage per label for {os.path.basename(video_path)}\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    hist_path = os.path.join(video_path, \"area_hist.png\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "    print(f\"üìä Saved histogram to {hist_path}\")\n",
    "\n",
    "    # --- Save CSV with percentages ---\n",
    "    import csv\n",
    "    csv_path = os.path.join(video_path,\"..\", \"segmentation_stats.csv\")\n",
    "    all_labels =  [\"grass\", \"trees\", \"bus\", \"building\",\"people\",\"car\",\"road\"]\n",
    "    values = [f\"{area_percent[l]:.3f}\" for l in labels]\n",
    "    file_exists = os.path.exists(csv_path)\n",
    "\n",
    "    with open(csv_path, \"a+\", newline=\"\") as f:\n",
    "        f.seek(0)\n",
    "        existing_rows = [row[0] for row in csv.reader(f)]\n",
    "        writer = csv.writer(f)\n",
    "        # Write header only once\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"video\"] + all_labels)\n",
    "        # write the values in order of the corredsponding labels\n",
    "        if os.path.basename(video_path) not in existing_rows:\n",
    "            #writer.writerow([os.path.basename(video_path)] + values)\n",
    "            writer.writerow([os.path.basename(video_path)] + [f\"{area_percent.get(label, 0):.3f}\" for label in all_labels])\n",
    "\n",
    "    print(f\"üíæ Created and saved histogram data to {csv_path}\")\n",
    "\n",
    "    # --- Return total percentage for labels of interest ---\n",
    "    return sum(area_percent.get(label, 0) for label in labels_of_interest)\n",
    "\n",
    "\n",
    "def map_videos(video_folders, labels_of_interest=[\"trees\", \"grass\"],out_filename=\"map.html\", ball_clr=\"green\"):\n",
    "    \"\"\"Plot circle markers for each video with radius ranked by tree area.\"\"\"\n",
    "    m = folium.Map(location=[38.736, -9.14], zoom_start=12)\n",
    "\n",
    "    # Step 1: Gather data\n",
    "    video_data = []\n",
    "    for video_name in tqdm(video_folders):\n",
    "        video_basename = os.path.basename(video_name)\n",
    "        group_dir = os.path.dirname(os.path.dirname(video_name))  # e.g., g1/output/topo_alameda ‚Üí g1/\n",
    "        video_path = os.path.join(group_dir, f\"{video_basename}.MOV\")\n",
    "\n",
    "        # fallback: try .mp4 if .MOV not found\n",
    "        if not os.path.exists(video_path):\n",
    "            video_path = os.path.join(group_dir, f\"{video_basename}.mp4\")\n",
    "\n",
    "        gps = get_gps_from_video(video_path)\n",
    "        area = hist_per_video(video_name, labels_of_interest)\n",
    "\n",
    "        if video_name == \"topo_alameda\":\n",
    "            gps = (38.737340, -9.129204)\n",
    "        elif gps is None:\n",
    "            print(f\"No GPS found for {video_name}\")\n",
    "            continue\n",
    "\n",
    "        # --- Save CSV with percentages ---\n",
    "        import csv\n",
    "        csv_path = os.path.join(video_name,\"..\", \"gps.csv\")\n",
    "        file_exists = os.path.exists(csv_path)\n",
    "\n",
    "        with open(csv_path, \"a+\", newline=\"\") as f:\n",
    "            f.seek(0)\n",
    "            existing_rows = [row[0] for row in csv.reader(f)]\n",
    "            writer = csv.writer(f)\n",
    "            # Write header only once\n",
    "            if not file_exists:\n",
    "                writer.writerow([\"video\"] + [\"lat\"] + [\"lon\"])\n",
    "            # Write only if the video name is not already in the file\n",
    "            if os.path.basename(video_name) not in existing_rows:\n",
    "                writer.writerow([os.path.basename(video_name)] + [gps[0]] + [gps[1]])\n",
    "\n",
    "        print(f\"üíæ Created and saved gps data to {csv_path}\")\n",
    "\n",
    "        video_data.append((video_name, gps, area))\n",
    "\n",
    "    if not video_data:\n",
    "        print(\"‚ö†Ô∏è No valid videos found.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Rank / normalize areas\n",
    "    areas = np.array([a for _, _, a in video_data])\n",
    "    min_area, max_area = np.min(areas), np.max(areas)\n",
    "    if max_area == min_area:\n",
    "        normalized = np.ones_like(areas)\n",
    "    else:\n",
    "        normalized = (areas - min_area) / (max_area - min_area)\n",
    "\n",
    "    # Step 3: Map normalized areas to a visual radius range\n",
    "    min_radius, max_radius = 10, 40\n",
    "    radii = min_radius + normalized * (max_radius - min_radius)\n",
    "\n",
    "    # Step 4: Plot circles with ranked sizes\n",
    "    for (video_name, (lat, lon), area), radius in zip(video_data, radii):\n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lon],\n",
    "            radius=radius,\n",
    "            color=ball_clr,\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "            popup=f\"{video_name}: {int(area)} px¬≤ trees\",\n",
    "        ).add_to(m)\n",
    "\n",
    "    m.save(out_filename)\n",
    "    print(f\"‚úÖ Saved map to {out_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \".\"\n",
    "#groups = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"g\")]\n",
    "groups = [\"g5\"]\n",
    "\n",
    "for group in groups:\n",
    "    output_root = os.path.join(base_dir, group)\n",
    "    if not os.path.isdir(output_root):\n",
    "        continue\n",
    "\n",
    "    # Collect all per-video output folders inside each group's output/\n",
    "    video_folders = [\n",
    "        os.path.join(output_root, d)\n",
    "        for d in os.listdir(output_root)\n",
    "        if os.path.isdir(os.path.join(output_root, d))\n",
    "    ]\n",
    "\n",
    "    if not video_folders:\n",
    "        print(f\"‚ö†Ô∏è No video outputs found in {group}/output\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÇ Processing {group}: {len(video_folders)} videos\")\n",
    "\n",
    "    # Run vegetation map\n",
    "    #map_videos(\n",
    "    #    video_folders,\n",
    "    #    labels_of_interest=[\"trees\", \"grass\"],\n",
    "    #    out_filename=os.path.join(output_root, f\"{group}_green_map.html\"),\n",
    "    #    ball_clr=\"green\",\n",
    "    #)\n",
    "\n",
    "    # Run urban map\n",
    "    map_videos(\n",
    "        video_folders,\n",
    "        labels_of_interest=[\"road\", \"car\", \"bus\", \"building\"],\n",
    "        out_filename=os.path.join(output_root, f\"{group}_urban_map.html\"),\n",
    "        ball_clr=\"grey\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"./g1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install k3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import k3d\n",
    "\n",
    "def visualize_vggt_pointcloud(video_path, downsample=10, conf_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Visualize VGGT 3D point cloud for a given video output folder.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): e.g. './g1/output/topo_alameda'\n",
    "        downsample (int): keep 1 every N points\n",
    "        conf_thresh (float): confidence threshold on world_points_conf\n",
    "    \"\"\"\n",
    "    vggt_dir = os.path.join(video_path, \"vggt\")\n",
    "    if not os.path.isdir(vggt_dir):\n",
    "        print(f\"‚ö†Ô∏è No VGGT folder in {video_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load the latest (or largest) pickle file\n",
    "    pkl_files = sorted(\n",
    "        [f for f in os.listdir(vggt_dir) if f.endswith(\".pkl\")],\n",
    "        key=lambda x: os.path.getsize(os.path.join(vggt_dir, x)),\n",
    "        reverse=True\n",
    "    )\n",
    "    if not pkl_files:\n",
    "        print(f\"‚ö†Ô∏è No VGGT pickle found in {vggt_dir}\")\n",
    "        return None\n",
    "\n",
    "    latest_pkl = os.path.join(vggt_dir, pkl_files[0])\n",
    "    print(f\"üì¶ Loading VGGT data from {latest_pkl}\")\n",
    "\n",
    "    with open(latest_pkl, \"rb\") as f:\n",
    "        vggt_data = pickle.load(f)\n",
    "\n",
    "    # --- Extract fields safely ---\n",
    "    world_points = np.array(vggt_data.get(\"world_points\") or vggt_data.get(\"wrld_points\"))\n",
    "    if world_points is None or world_points.size == 0:\n",
    "        print(f\"‚ö†Ô∏è No 'world_points' field found in {latest_pkl}\")\n",
    "        return None\n",
    "\n",
    "    conf = np.array(vggt_data.get(\"world_points_conf\", np.ones(world_points.shape[:-1])))\n",
    "    conf = np.squeeze(conf)\n",
    "\n",
    "    # --- Apply confidence mask ---\n",
    "    mask_conf = conf > conf_thresh\n",
    "    world_points = np.reshape(world_points, (-1, 3))[mask_conf.flatten(), :]\n",
    "\n",
    "    # --- Handle RGB ---\n",
    "    images = vggt_data.get(\"images\")\n",
    "    if images is not None:\n",
    "        # images: (N, C, H, W) ‚Üí (H*W*N, 3)\n",
    "        rgb = np.transpose(images, (0, 2, 3, 1)).reshape(-1, 3)\n",
    "        rgb = np.clip(rgb * 255.0, 0, 255).astype(np.uint8)[mask_conf.flatten(), :]\n",
    "    else:\n",
    "        rgb = np.full((world_points.shape[0], 3), 200, dtype=np.uint8)  # gray fallback\n",
    "\n",
    "    # --- Downsample for speed ---\n",
    "    if downsample > 1:\n",
    "        world_points = world_points[::downsample]\n",
    "        rgb = rgb[::downsample]\n",
    "\n",
    "    # --- Convert to integer RGB for K3D ---\n",
    "    rgb_int = (\n",
    "        (rgb[:, 0].astype(np.uint32) << 16) +\n",
    "        (rgb[:, 1].astype(np.uint32) << 8) +\n",
    "        rgb[:, 2].astype(np.uint32)\n",
    "    )\n",
    "\n",
    "    # --- Visualize in K3D ---\n",
    "    plot = k3d.plot(height=600)\n",
    "    point_cloud = k3d.points(\n",
    "        positions=world_points,\n",
    "        colors=rgb_int,\n",
    "        point_size=0.005,\n",
    "        shader='3d'\n",
    "    )\n",
    "    plot += point_cloud\n",
    "    plot.display()\n",
    "\n",
    "    print(f\"‚úÖ Displayed {len(world_points):,} points from {os.path.basename(video_path)}\")\n",
    "    return plot\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "video = \"./g1/output/almirante_reis\"\n",
    "plot = visualize_vggt_pointcloud(video, downsample=10, conf_thresh=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Processing ./g5/IMG_8619 ...\n",
      "üì¶ Processing ./g5/IMG_8599 ...\n",
      "üì¶ Processing ./g5/IMG_8589 ...\n",
      "üì¶ Processing ./g5/IMG_8684 ...\n",
      "üì¶ Processing ./g5/IMG_8597 ...\n",
      "üì¶ Processing ./g5/IMG_8607 ...\n",
      "üì¶ Processing ./g5/IMG_8676 ...\n",
      "üì¶ Processing ./g5/IMG_8643 ...\n",
      "üì¶ Processing ./g5/IMG_8638 ...\n",
      "üì¶ Processing ./g5/IMG_8688 ...\n",
      "üì¶ Processing ./g5/IMG_8642 ...\n",
      "üì¶ Processing ./g5/IMG_8672 ...\n",
      "üì¶ Processing ./g5/IMG_8671 ...\n",
      "üì¶ Processing ./g5/IMG_8610 ...\n",
      "üì¶ Processing ./g5/IMG_8641 ...\n",
      "üì¶ Processing ./g5/IMG_8646 ...\n",
      "üì¶ Processing ./g5/IMG_8581 ...\n",
      "üì¶ Processing ./g5/IMG_8658 ...\n",
      "üì¶ Processing ./g5/IMG_8679 ...\n",
      "üì¶ Processing ./g5/IMG_8686 ...\n",
      "üì¶ Processing ./g5/IMG_8588 ...\n",
      "üì¶ Processing ./g5/IMG_8630 ...\n",
      "üì¶ Processing ./g5/IMG_8680 ...\n",
      "üì¶ Processing ./g5/IMG_8625 ...\n",
      "üì¶ Processing ./g5/IMG_8670 ...\n",
      "üì¶ Processing ./g5/IMG_8666 ...\n",
      "üì¶ Processing ./g5/IMG_8629 ...\n",
      "üì¶ Processing ./g5/IMG_8586 ...\n",
      "üì¶ Processing ./g5/IMG_8627 ...\n",
      "üì¶ Processing ./g5/IMG_8675 ...\n",
      "üì¶ Processing ./g5/IMG_8674 ...\n",
      "üì¶ Processing ./g5/IMG_8635 ...\n",
      "üì¶ Processing ./g5/IMG_8637 ...\n",
      "üì¶ Processing ./g5/IMG_8603 ...\n",
      "üì¶ Processing ./g5/IMG_8584 ...\n",
      "üì¶ Processing ./g5/IMG_8667 ...\n",
      "üì¶ Processing ./g5/IMG_8605 ...\n",
      "üì¶ Processing ./g5/IMG_8683 ...\n",
      "üì¶ Processing ./g5/IMG_8663 ...\n",
      "üì¶ Processing ./g5/IMG_8615 ...\n",
      "üì¶ Processing ./g5/IMG_8617 ...\n",
      "üì¶ Processing ./g5/IMG_8690 ...\n",
      "üì¶ Processing ./g5/IMG_8650 ...\n",
      "üì¶ Processing ./g5/IMG_8608 ...\n",
      "üì¶ Processing ./g5/IMG_8621 ...\n",
      "üì¶ Processing ./g5/IMG_8677 ...\n",
      "üì¶ Processing ./g5/IMG_8673 ...\n",
      "üì¶ Processing ./g5/IMG_8645 ...\n",
      "üì¶ Processing ./g5/IMG_8689 ...\n",
      "üì¶ Processing ./g5/IMG_8662 ...\n",
      "üì¶ Processing ./g5/IMG_8653 ...\n",
      "üì¶ Processing ./g5/IMG_8661 ...\n",
      "üì¶ Processing ./g5/IMG_8693 ...\n",
      "üì¶ Processing ./g5/IMG_8647 ...\n",
      "üì¶ Processing ./g5/IMG_8616 ...\n",
      "üì¶ Processing ./g5/IMG_8583 ...\n",
      "üì¶ Processing ./g5/IMG_8648 ...\n",
      "üì¶ Processing ./g5/IMG_8595 ...\n",
      "üì¶ Processing ./g5/IMG_8640 ...\n",
      "üì¶ Processing ./g5/IMG_8644 ...\n",
      "üì¶ Processing ./g5/IMG_8639 ...\n",
      "üì¶ Processing ./g5/IMG_8687 ...\n",
      "üì¶ Processing ./g5/IMG_8656 ...\n",
      "üì¶ Processing ./g5/IMG_8604 ...\n",
      "üì¶ Processing ./g5/IMG_8678 ...\n",
      "üì¶ Processing ./g5/IMG_8611 ...\n",
      "üì¶ Processing ./g5/IMG_8628 ...\n",
      "üì¶ Processing ./g5/IMG_8654 ...\n",
      "üì¶ Processing ./g5/IMG_8600 ...\n",
      "üì¶ Processing ./g5/IMG_8657 ...\n",
      "üì¶ Processing ./g5/IMG_8659 ...\n",
      "üì¶ Processing ./g5/IMG_8682 ...\n",
      "üì¶ Processing ./g5/IMG_8631 ...\n",
      "üì¶ Processing ./g5/IMG_8660 ...\n",
      "üì¶ Processing ./g5/IMG_8664 ...\n",
      "üì¶ Processing ./g5/IMG_8598 ...\n",
      "üì¶ Processing ./g5/IMG_8649 ...\n",
      "üì¶ Processing ./g5/IMG_8601 ...\n",
      "üì¶ Processing ./g5/IMG_8596 ...\n",
      "üì¶ Processing ./g5/IMG_8624 ...\n",
      "üì¶ Processing ./g5/IMG_8669 ...\n",
      "üì¶ Processing ./g5/IMG_8618 ...\n",
      "üì¶ Processing ./g5/IMG_8622 ...\n",
      "üì¶ Processing ./g5/IMG_8623 ...\n",
      "üì¶ Processing ./g5/IMG_8620 ...\n",
      "üì¶ Processing ./g5/IMG_8655 ...\n",
      "üì¶ Processing ./g5/IMG_8685 ...\n",
      "üì¶ Processing ./g5/IMG_8652 ...\n",
      "üì¶ Processing ./g5/IMG_8614 ...\n",
      "üì¶ Processing ./g5/IMG_8613 ...\n",
      "üì¶ Processing ./g5/IMG_8692 ...\n",
      "üì¶ Processing ./g5/IMG_8665 ...\n",
      "üì¶ Processing ./g5/IMG_8668 ...\n",
      "üì¶ Processing ./g5/IMG_8606 ...\n",
      "üì¶ Processing ./g5/IMG_8636 ...\n",
      "‚úÖ Saved ./g5/yolo_counts.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>traffic light</th>\n",
       "      <th>person</th>\n",
       "      <th>bench</th>\n",
       "      <th>stop sign</th>\n",
       "      <th>bicycle</th>\n",
       "      <th>truck</th>\n",
       "      <th>bus</th>\n",
       "      <th>motorcycle</th>\n",
       "      <th>fire hydrant</th>\n",
       "      <th>chair</th>\n",
       "      <th>parking meter</th>\n",
       "      <th>refrigerator</th>\n",
       "      <th>total_objects</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IMG_8581</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_8583</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_8584</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_8586</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_8588</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_8686</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_8688</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_8690</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_8692</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_8693</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          car  traffic light  person  bench  stop sign  bicycle  truck  bus  \\\n",
       "video_id                                                                      \n",
       "IMG_8581    1              0      12      0          0        0      0    1   \n",
       "IMG_8583    1              0       1      0          0        1      0    0   \n",
       "IMG_8584    7              0       0      0          0        0      0    0   \n",
       "IMG_8586    4              0       1      1          0        0      0    0   \n",
       "IMG_8588    0              0       3      0          0        0      0    0   \n",
       "...       ...            ...     ...    ...        ...      ...    ...  ...   \n",
       "IMG_8686    7              0       0      0          0        0      0    0   \n",
       "IMG_8688    1              0       0      0          0        0      0    0   \n",
       "IMG_8690    0              0       0      0          0        0      0    2   \n",
       "IMG_8692    5              0       0      0          0        0      0    0   \n",
       "IMG_8693    0              0       2      0          0        0      0    0   \n",
       "\n",
       "          motorcycle  fire hydrant  chair  parking meter  refrigerator  \\\n",
       "video_id                                                                 \n",
       "IMG_8581           0             0      0              0             0   \n",
       "IMG_8583           0             0      0              0             0   \n",
       "IMG_8584           0             0      0              0             0   \n",
       "IMG_8586           0             0      0              0             0   \n",
       "IMG_8588           0             0      0              0             0   \n",
       "...              ...           ...    ...            ...           ...   \n",
       "IMG_8686           0             0      0              0             0   \n",
       "IMG_8688           0             0      0              0             0   \n",
       "IMG_8690           0             0      0              0             0   \n",
       "IMG_8692           0             0      0              0             0   \n",
       "IMG_8693           0             0      0              0             0   \n",
       "\n",
       "          total_objects  \n",
       "video_id                 \n",
       "IMG_8581             14  \n",
       "IMG_8583              3  \n",
       "IMG_8584              7  \n",
       "IMG_8586              6  \n",
       "IMG_8588              3  \n",
       "...                 ...  \n",
       "IMG_8686              7  \n",
       "IMG_8688              1  \n",
       "IMG_8690              2  \n",
       "IMG_8692              5  \n",
       "IMG_8693              2  \n",
       "\n",
       "[84 rows x 14 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_objects_in_video(video_yolo_json_dir, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Count unique detected objects per class in a YOLO JSON folder,\n",
    "    considering only detections above a confidence threshold.\n",
    "    \"\"\"\n",
    "    counts = defaultdict(set)  # {class_name: set(track_ids)}\n",
    "    json_files = sorted([f for f in os.listdir(video_yolo_json_dir) if f.endswith(\".json\")])\n",
    "\n",
    "    for jf in json_files:\n",
    "        jf_path = os.path.join(video_yolo_json_dir, jf)\n",
    "        try:\n",
    "            with open(jf_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read {jf_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if not data or not isinstance(data, list):\n",
    "            continue\n",
    "\n",
    "        detections = data[0] if isinstance(data[0], list) else data\n",
    "        for det in detections:\n",
    "            conf = det.get(\"confidence\", 0.0)\n",
    "            if conf < conf_threshold:\n",
    "                continue\n",
    "\n",
    "            cls = det.get(\"class_name\", \"unknown\")\n",
    "            tid = det.get(\"track_id\", None)\n",
    "\n",
    "            # Handle missing or invalid tracking IDs\n",
    "            if tid is None or tid == -1:\n",
    "                tid = f\"{jf}_{cls}\"  # fallback unique id per frame\n",
    "\n",
    "            counts[cls].add(tid)\n",
    "\n",
    "    return {cls: len(tids) for cls, tids in counts.items()}\n",
    "\n",
    "def process_group(group_dir, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Process all videos inside a group/output/ folder and save a CSV there.\n",
    "    \"\"\"\n",
    "    output_dir = os.path.join(group_dir)\n",
    "    if not os.path.isdir(output_dir):\n",
    "        print(f\"‚ö†Ô∏è No output folder in {group_dir}\")\n",
    "        return\n",
    "\n",
    "    video_stats = {}\n",
    "    for video_name in os.listdir(output_dir):\n",
    "        video_dir = os.path.join(output_dir, video_name)\n",
    "        yolo_json_dir = os.path.join(video_dir, \"yolo_json\")\n",
    "        if not os.path.isdir(yolo_json_dir):\n",
    "            continue\n",
    "\n",
    "        print(f\"üì¶ Processing {group_dir}/{video_name} ...\")\n",
    "        counts = count_objects_in_video(yolo_json_dir, conf_threshold)\n",
    "        if counts:\n",
    "            video_stats[video_name] = counts\n",
    "\n",
    "    if not video_stats:\n",
    "        print(f\"‚ö†Ô∏è No valid YOLO data found in {group_dir}\")\n",
    "        return\n",
    "\n",
    "    # Normalize into DataFrame\n",
    "    df = pd.DataFrame.from_dict(video_stats, orient=\"index\").fillna(0).astype(int)\n",
    "    df.index.name = \"video_id\"\n",
    "    df.sort_index(inplace=True)\n",
    "    df[\"total_objects\"] = df.sum(axis=1)\n",
    "\n",
    "    csv_path = os.path.join(output_dir, f\"yolo_counts.csv\")\n",
    "    df.to_csv(csv_path)\n",
    "    print(f\"‚úÖ Saved {csv_path}\")\n",
    "    return df\n",
    "\n",
    "def merge_all_groups(base_dir=\".\", conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Loop over all group folders (g1, g2, g5-nogps, etc.) and create one CSV per group.\n",
    "    \"\"\"\n",
    "    for group in sorted(os.listdir(base_dir)):\n",
    "        group_path = os.path.join(base_dir, group)\n",
    "        if not os.path.isdir(group_path) or not group.startswith(\"g\"):\n",
    "            continue\n",
    "        process_group(group_path, conf_threshold)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    merge_all_groups(\".\", conf_threshold=0.65)\n",
    "\n",
    "process_group(\"./g5\", 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def dms_to_decimal(degrees, minutes, seconds, direction):\n",
    "    \"\"\"Convert DMS to decimal degrees.\"\"\"\n",
    "    decimal = float(degrees) + float(minutes)/60 + float(seconds)/3600\n",
    "    if direction in ['S', 'W']:\n",
    "        decimal *= -1\n",
    "    return decimal\n",
    "\n",
    "def parse_videos(text):\n",
    "    pattern = re.compile(\n",
    "        r\"V√≠deo\\s*(\\d+)\\s*[‚Äì-]\\s*\"\n",
    "        r\"(\\d+)¬∞(\\d+)‚Äô(\\d+)‚Äù([NS])?\\s*\"\n",
    "        r\"(\\d+)¬∞(\\d+)‚Äô(\\d+)‚Äù([EW])?\"\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "    for match in pattern.finditer(text):\n",
    "        vid, lat_d, lat_m, lat_s, lat_dir, lon_d, lon_m, lon_s, lon_dir = match.groups()\n",
    "        lat_dir = lat_dir or 'N'\n",
    "        lon_dir = lon_dir or 'W'\n",
    "        lat = dms_to_decimal(lat_d, lat_m, lat_s, lat_dir)\n",
    "        lon = dms_to_decimal(lon_d, lon_m, lon_s, lon_dir)\n",
    "        data.append({\"video\": int(vid), \"latitude\": lat, \"longitude\": lon})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example usage:\n",
    "with open(\"coords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "df = parse_videos(text)\n",
    "df.to_csv(\"video_coordinates.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.716111</td>\n",
       "      <td>-9.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38.717778</td>\n",
       "      <td>-9.144167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>38.717222</td>\n",
       "      <td>-9.143611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>38.716944</td>\n",
       "      <td>-9.143056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>38.716389</td>\n",
       "      <td>-9.142778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>70</td>\n",
       "      <td>38.718611</td>\n",
       "      <td>-9.143611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>71</td>\n",
       "      <td>38.716944</td>\n",
       "      <td>-9.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>72</td>\n",
       "      <td>38.717500</td>\n",
       "      <td>-9.142778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>73</td>\n",
       "      <td>38.716389</td>\n",
       "      <td>-9.142222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>74</td>\n",
       "      <td>38.716389</td>\n",
       "      <td>-9.141944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    video   latitude  longitude\n",
       "0       1  38.716111  -9.142500\n",
       "1       2  38.717778  -9.144167\n",
       "2       3  38.717222  -9.143611\n",
       "3       4  38.716944  -9.143056\n",
       "4       5  38.716389  -9.142778\n",
       "..    ...        ...        ...\n",
       "68     70  38.718611  -9.143611\n",
       "69     71  38.716944  -9.142500\n",
       "70     72  38.717500  -9.142778\n",
       "71     73  38.716389  -9.142222\n",
       "72     74  38.716389  -9.141944\n",
       "\n",
       "[73 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
